{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11186b2a-76fc-425d-a8eb-fcaee4ab9a5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PEI-Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "737a5088-f8e2-40fd-95fc-0800a01c7467",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, LongType, IntegerType, StringType, DecimalType, DateType, TimestampType, DoubleType\n",
    "from pyspark.sql.functions import udf, to_date, col, to_timestamp\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Assessment\").getOrCreate()\n",
    "\n",
    "# Install Maven library from configuration \"com.crealytics:spark-excel_2.12:0.13.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54e70b6c-67de-4005-a537-8b7b73362771",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to remove spaces and hypen from column names\n",
    "def col_name_formatter(df):\n",
    "    for col_name in df.columns:\n",
    "        if ' ' in col_name or '-' in col_name:\n",
    "            df = df.withColumnRenamed(col_name, col_name.replace(' ', '_').replace('-', '_'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6ce3970-8200-45ee-a187-a8c0231f21dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c14655-32ed-4403-8214-35b6ef36a248",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to format price - Extraction of floating point numbers\n",
    "def format_price(price):\n",
    "    # price = str(price)\n",
    "    cleaned_price = re.findall(r'\\b\\d+(?:\\.\\d+)?\\b', price)\n",
    "    # if cleaned_price:\n",
    "    #     return cleaned_price[0]\n",
    "    return cleaned_price[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fbb9a6b-dc19-447c-9f2b-3a6ce9666e5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+--------------+-------+---------------+-------+--------+------+----------+--------------+\n|Customer_ID|Discount|Order_Date|Order_ID      |Price  |Product_ID     |Profit |Quantity|Row_ID|Ship_Date |Ship_Mode     |\n+-----------+--------+----------+--------------+-------+---------------+-------+--------+------+----------+--------------+\n|JK-15370   |0.3     |2016-08-21|CA-2016-122581|573.174|FUR-CH-10002961|63.686 |7       |1     |2016-08-25|Standard Class|\n|BD-11320   |0.0     |2017-09-23|CA-2017-117485|291.96 |TEC-AC-10004659|102.186|4       |2     |2017-09-29|Standard Class|\n|LB-16795   |0.7     |2016-10-06|US-2016-157490|17.0   |OFF-BI-10002824|-14.92 |4       |3     |2016-10-07|First Class   |\n|KB-16315   |0.2     |2015-07-02|CA-2015-111703|15.552 |OFF-PA-10003349|5.6376 |3       |4     |2015-07-09|Standard Class|\n|DO-13435   |0.2     |2014-10-03|CA-2014-108903|142.488|TEC-AC-10003023|-3.0   |3       |5     |2014-10-03|Same Day      |\n|CB-12025   |0.0     |2016-11-27|CA-2016-117583|79.95  |OFF-BI-10004233|38.376 |5       |6     |2016-11-30|First Class   |\n|SM-20005   |0.0     |2014-12-10|CA-2014-148488|11.0   |OFF-PA-10004470|5.2256 |2       |7     |2014-12-15|Standard Class|\n|RD-19480   |0.0     |2016-12-01|CA-2016-136434|17.31  |FUR-FU-10001196|5.193  |3       |8     |2016-12-07|Standard Class|\n|JM-16195   |0.0     |2014-04-30|CA-2014-160094|826.0  |OFF-ST-10000585|214.0  |5       |9     |2014-05-02|First Class   |\n|SC-20230   |0.0     |2017-08-03|CA-2017-141747|16.06  |OFF-ST-10003996|4.1756 |1       |10    |2017-08-08|Second Class  |\n|BO-11350   |0.2     |2017-05-03|CA-2017-132199|8.0    |OFF-FA-10002280|2.8    |2       |11    |2017-05-08|Standard Class|\n|BD-11320   |0.2     |2017-11-27|CA-2017-107125|117.488|OFF-BI-10001989|41.1208|7       |12    |2017-12-02|Standard Class|\n|AB-10105   |0.7     |2017-09-19|CA-2017-153822|18.18  |OFF-BI-10001460|-13.938|4       |13    |2017-09-25|Standard Class|\n|NP-18670   |0.0     |2017-10-12|CA-2017-150091|45.0   |TEC-AC-10002167|4.0    |3       |14    |2017-10-16|Standard Class|\n|KD-16270   |0.0     |2016-09-02|CA-2016-130407|39.98  |FUR-FU-10001967|9.995  |2       |15    |2016-09-06|Standard Class|\n|BF-11005   |0.6     |2016-07-28|US-2016-105452|302.0  |FUR-FU-10003806|-378.4 |5       |16    |2016-08-01|Standard Class|\n|LE-16810   |0.8     |2014-05-27|US-2014-117058|17.46  |OFF-BI-10004139|-30.555|6       |17    |2014-05-30|First Class   |\n|TT-21070   |0.0     |2017-11-13|CA-2017-122490|344.91 |OFF-ST-10000991|10.3473|3       |18    |2017-11-18|Standard Class|\n|CA-12055   |0.2     |2016-11-22|US-2016-164945|134.272|OFF-BI-10001524|46.9952|8       |19    |2016-11-27|Standard Class|\n|GD-14590   |0.0     |2014-05-05|CA-2014-111934|11.88  |OFF-BI-10004364|5.0    |2       |20    |2014-05-07|First Class   |\n+-----------+--------+----------+--------------+-------+---------------+-------+--------+------+----------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Test col_name_formatter function\n",
    "def test_col_name_formatter(spark, df_order):\n",
    "    df_order = col_name_formatter(df_order)\n",
    "    expected_columns = ['Customer_ID', 'Discount', 'Order_Date', 'Order_ID', 'Price', 'Product_ID', 'Profit', 'Quantity', 'Row_ID', 'Ship_Date', 'Ship_Mode']\n",
    "    assert df_order.columns == expected_columns, \"Columns are not formatted correctly\"\n",
    "    \n",
    "    return df_order\n",
    "  \n",
    "# Test casting column to LongType\n",
    "def test_cast_column(df_order):\n",
    "    df_order = df_order.withColumn(\"Price\", col(\"Price\").cast(DoubleType()))\n",
    "    expected_dtype = \"double\"\n",
    "    assert df_order.schema[\"Price\"].dataType.typeName() == expected_dtype, \"Price column cast to DoubleType failed\"\n",
    "\n",
    "    df_order = df_order.withColumn(\"Discount\", col(\"Discount\").cast(DoubleType()))\n",
    "    expected_dtype = \"double\"\n",
    "    assert df_order.schema[\"Discount\"].dataType.typeName() == expected_dtype, \"Discount column cast to DoubleType failed\"\n",
    "\n",
    "    df_order = df_order.withColumn(\"Profit\", col(\"Profit\").cast(DoubleType()))\n",
    "    expected_dtype = \"double\"\n",
    "    assert df_order.schema[\"Profit\"].dataType.typeName() == expected_dtype, \"Profit column cast to DoubleType failed\"\n",
    "\n",
    "    df_order = df_order.withColumn(\"Quantity\", col(\"Quantity\").cast(LongType()))\n",
    "    expected_dtype = \"long\"\n",
    "    assert df_order.schema[\"Quantity\"].dataType.typeName() == expected_dtype, \"Quantity column cast to LongType failed\"\n",
    "    \n",
    "    df_order = df_order.withColumn(\"Row_ID\", col(\"Row_ID\").cast(LongType()))\n",
    "    expected_dtype = \"long\"\n",
    "    assert df_order.schema[\"Row_ID\"].dataType.typeName() == expected_dtype, \"Row_ID column cast to LongType failed\"\n",
    "\n",
    "    df_order = df_order.withColumn(\"Order_Date\", col(\"Order_Date\").cast(DateType()))\n",
    "    expected_dtype = \"date\"\n",
    "    assert df_order.schema[\"Order_Date\"].dataType.typeName() == expected_dtype, \"Order_Date column cast to DateType failed\"\n",
    "    \n",
    "    df_order = df_order.withColumn(\"Ship_Date\", col(\"Ship_Date\").cast(DateType()))\n",
    "    expected_dtype = \"date\"\n",
    "    assert df_order.schema[\"Ship_Date\"].dataType.typeName() == expected_dtype, \"Ship_Date column cast to DateType failed\"\n",
    "\n",
    "    return df_order\n",
    "\n",
    "# Test 1: col_name_formatter\n",
    "df_order = spark.read.option(\"multiline\",\"true\").json(\"dbfs:/FileStore/PEI-Assessment/Order.json\")\n",
    "df_order = test_col_name_formatter(spark, df_order)\n",
    "\n",
    "\n",
    "# display(df_order)\n",
    "\n",
    "format_price_udf = udf(format_price, StringType())\n",
    "df_order = df_order.withColumn(\"Price\", format_price_udf(\"Price\"))\n",
    "df_order = df_order.withColumn(\"Price\", col(\"Price\").cast(DoubleType()))\n",
    "\n",
    "df_order = df_order.withColumn(\"Order_Date\", to_date(to_timestamp(col(\"Order_Date\"), \"d/M/yyyy\")))\n",
    "df_order = df_order.withColumn(\"Ship_Date\", to_date(to_timestamp(col(\"Ship_Date\"), \"d/M/yyyy\")))\n",
    "df_order = df_order.withColumn(\"Discount\", col(\"Discount\").cast(DoubleType()))\n",
    "df_order = df_order.withColumn(\"Profit\", col(\"Profit\").cast(DoubleType()))\n",
    "df_order = df_order.withColumn(\"Quantity\", col(\"Quantity\").cast(LongType()))\n",
    "df_order = df_order.withColumn(\"Row_ID\", col(\"Row_ID\").cast(LongType()))\n",
    "\n",
    "# Test 2: Cast column to LongType\n",
    "df_order = test_cast_column(df_order)\n",
    "\n",
    "# display(df_order)\n",
    "df_order.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4017c38-1ff3-48b1-a769-47e4f846c5a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7ef4cd5-302c-4061-a43c-4d6363d3eea8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def format_customer_name(customer_name):\n",
    "    if not customer_name:\n",
    "        return None\n",
    "    \n",
    "    # Remove all non-alphabetic characters between two lowercase alphabets\n",
    "    cleaned_name = re.sub(r'([a-z])[^a-zA-Z]+([a-z])', r'\\1\\2', customer_name)\n",
    "    \n",
    "    # Remove all non-alphabetic characters between uppercase and lowercase alphabets\n",
    "    cleaned_name = re.sub(r'([A-Z])[^a-zA-Z]+([a-z])', r'\\1\\2', cleaned_name)\n",
    "\n",
    "    # Remove all non-alphabetic characters between a lowercase and an uppercase alphabet and add one white space in between\n",
    "    cleaned_name = re.sub(r'([a-z])([^a-zA-Z]+)([A-Z])', r'\\1 \\3', cleaned_name)\n",
    "\n",
    "    # Remove all leading non-alphabetic characters\n",
    "    cleaned_name = re.sub(r'^[^a-zA-Z]+|[^a-zA-Z]+$', '', cleaned_name)\n",
    "\n",
    "    if cleaned_name.strip() == '':\n",
    "        return None\n",
    "    return cleaned_name.strip()\n",
    "\n",
    "def format_phone_number(phone_number):\n",
    "\n",
    "    if not phone_number:\n",
    "        return None\n",
    "    phone_number = str(phone_number)\n",
    "    # Remove everything after x\n",
    "    phone_number = phone_number.split('x')[0]\n",
    "\n",
    "    # Remove non-numeric characters from the phone number\n",
    "    numeric_phone_number = re.sub(r'\\D', '', phone_number)\n",
    "\n",
    "    # Remove leading '001'\n",
    "    numeric_phone_number = re.sub(r'^001', '', numeric_phone_number)\n",
    "    \n",
    "    numeric_phone_number= numeric_phone_number.lstrip('0')\n",
    "    if len(numeric_phone_number) != 10:\n",
    "        return None\n",
    "    return numeric_phone_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bdcf441-41f6-4c10-8e71-60b4be47eb73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-----------------------------+----------+----------------------------------------------------+-----------+-------------+----------------+------------+-----------+-------+\n|Customer_ID|Customer_Name     |email                        |Phone     |address                                             |Segment    |Country      |City            |State       |Postal_Code|Region |\n+-----------+------------------+-----------------------------+----------+----------------------------------------------------+-----------+-------------+----------------+------------+-----------+-------+\n|PW-19240   |Pierre Wener      |bettysullivan808@gmail.com   |4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462   |Consumer   |United States|Louisville      |Colorado    |80027      |West   |\n|GH-14410   |Gary Hansen       |austindyer948@gmail.com      |5424150246|00347 Murphy Unions\\nAshleyton, IA 29814            |Home Office|United States|Chicago         |Illinois    |60653      |Central|\n|KL-16555   |Kelly Lampkin     |clarencehughes280@gmail.com  |7185624866|007 Adams Lane Suite 176\\nEast Amyberg, IN 34581    |Corporate  |United States|Colorado Springs|Colorado    |80906      |West   |\n|AH-10075   |Adam Hart         |angelabryant256@gmail.com    |2651015569|01454 Christopher Turnpike\\nNorth Ryanstad, MI 36226|Corporate  |United States|Columbus        |Ohio        |43229      |East   |\n|PF-19165   |Philip Fox        |kristinereynolds576@gmail.com|4736452141|0158 Harris Ways Suite 085\\nEast Laceyside, SD 35649|Consumer   |United States|San Diego       |California  |92105      |West   |\n|SC-20680   |Steve Carroll     |jasoncontreras178@gmail.com  |5636474830|01630 Tammy Prairie\\nNorth Daniel, KS 26404         |Home Office|United States|Seattle         |Washington  |98105      |West   |\n|JR-15700   |Jocasta Rupert    |johncombs689@gmail.com       |NULL      |019 Emily Corner Apt. 810\\nRyantown, SC 37010       |Consumer   |United States|Jacksonville    |Florida     |32216      |South  |\n|AB-10105   |Adrian Barton     |daviddavis980@gmail.com      |NULL      |021 Katherine Mall\\nJameston, DC 24685              |Consumer   |United States|Phoenix         |Arizona     |85023      |West   |\n|PT-19090   |Pete Takahito     |mikaylaarnold666@gmail.com   |7866386820|0236 Lane Squares\\nPort Samantha, ME 15670          |Consumer   |United States|San Antonio     |Texas       |78207      |Central|\n|SG-20605   |Speros Goranitis  |brianjoyce110@gmail.com      |3528465094|02401 Angela Loop Apt. 678\\nPort John, ME 43448     |Consumer   |United States|Lafayette       |Indiana     |47905      |Central|\n|MH-17785   |Maya Herman       |christinasalas345@gmail.com  |7223765599|026 Colon Hill\\nNew Pedromouth, CA 18437            |Corporate  |United States|San Diego       |California  |92105      |West   |\n|KB-16240   |Karen Bern        |christopherperez199@gmail.com|8174090760|026 White Squares\\nRobertton, VA 78741              |Corporate  |United States|Odessa          |Texas       |79762      |Central|\n|JM-15535   |Jessica Myrick    |nicholasrussell869@gmail.com |9244847935|02785 Johnson Shore\\nSouth Nathan, KY 45544         |Consumer   |United States|New York City   |New York    |10035      |East   |\n|JH-16180   |Justin Hirsh      |kellymartinez374@gmail.com   |3617206500|0317 Parker Lane\\nPort Jennifer, OK 19258           |Consumer   |United States|Philadelphia    |Pennsylvania|19140      |East   |\n|CS-12400   |Christopher Schild|matthewwilliams637@gmail.com |1062295443|034 Lynch Squares\\nNorth Benjaminside, MH 42764     |Home Office|United States|Philadelphia    |Pennsylvania|19134      |East   |\n|LR-17035   |Lisa Ryan         |lorimorrow317@gmail.com      |5497075809|035 Cox View Suite 514\\nEast Carlton, VA 31206      |Corporate  |United States|Santa Clara     |California  |95051      |West   |\n|TW-21025   |Tamara Willingham |selenajones479@gmail.com     |2323700362|03551 Michael Shore\\nLake Barryview, MH 82312       |Home Office|United States|Charlottesville |Virginia    |22901      |South  |\n|SR-20425   |Sharelle Roach    |raymondpeck288@gmail.com     |8973627773|03958 Shane Lakes\\nNew Jessicatown, AR 78525        |Home Office|United States|Louisville      |Colorado    |80027      |West   |\n|AS-10285   |Alejandro Savely  |laceymercado410@gmail.com    |2167104605|04225 Lee Manor\\nCastroville, HI 72528              |Corporate  |United States|San Francisco   |California  |94109      |West   |\n|KB-16405   |Katrina Bavinger  |michaelwood756@gmail.com     |3478669161|0432 Jennifer Port Suite 897\\nJohnstad, AR 12201    |Home Office|United States|Apple Valley    |California  |92307      |West   |\n+-----------+------------------+-----------------------------+----------+----------------------------------------------------+-----------+-------------+----------------+------------+-----------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Test col_name_formatter function\n",
    "def test_col_name_formatter(spark, df_customer):\n",
    "    df_customer = col_name_formatter(df_customer)\n",
    "    expected_columns = ['Customer_ID', 'Customer_Name', 'email', 'phone', 'address', 'Segment', 'Country', 'City', 'State', 'Postal_Code', 'Region']\n",
    "    assert df_customer.columns == expected_columns, \"Columns are not formatted correctly\"\n",
    "    \n",
    "    return df_customer\n",
    "  \n",
    "# Test casting column to LongType\n",
    "def test_cast_column_to_long(df_customer):\n",
    "    df_customer = df_customer.withColumn(\"Phone\", col(\"Phone\").cast(LongType()))\n",
    "    expected_dtype = \"long\"\n",
    "    assert df_customer.schema[\"Phone\"].dataType.typeName() == expected_dtype, \"Phone column cast to LongType failed\"\n",
    "    \n",
    "    df_customer = df_customer.withColumn(\"Postal_Code\", col(\"Postal_Code\").cast(LongType()))\n",
    "    expected_dtype = \"long\"\n",
    "    assert df_customer.schema[\"Postal_Code\"].dataType.typeName() == expected_dtype, \"Postal_Code column cast to LongType failed\"\n",
    "\n",
    "    return df_customer\n",
    "\n",
    "# Test 1: col_name_formatter\n",
    "df_customer = spark.read.format(\"com.crealytics.spark.excel\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"dataAddress\", \"Worksheet\").load(\"dbfs:/FileStore/PEI-Assessment/Customer.xlsx\")\n",
    "df_customer = test_col_name_formatter(spark, df_customer)\n",
    "\n",
    "# display(df_customer)\n",
    "\n",
    "\n",
    "# Call UDF to format Customer_Name\n",
    "format_customer_name_udf = udf(format_customer_name, StringType())\n",
    "df_customer = df_customer.withColumn(\"Customer_Name\", format_customer_name_udf(\"Customer_Name\"))\n",
    "\n",
    "# Call UDF to format Phone number\n",
    "\n",
    "format_phone_number_udf = udf(format_phone_number, StringType())\n",
    "df_customer = df_customer.withColumn(\"Phone\", format_phone_number_udf(\"Phone\"))\n",
    "df_customer = df_customer.withColumn(\"Phone\", col('Phone').cast(LongType()))\n",
    "df_customer = df_customer.withColumn(\"Postal_Code\", col('Postal_Code').cast(LongType()))\n",
    "\n",
    "# Test 2: Cast column to LongType\n",
    "df_customer = test_cast_column_to_long(df_customer)\n",
    "\n",
    "# display(df_customer)\n",
    "df_customer.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7e681fa-0a7e-43d5-9be4-7c8437f8a547",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8fd0477-d0e0-4968-aa0b-8a4a98928b00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "us_states = {\n",
    "    'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'florida', 'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine', 'maryland', 'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska', 'nevada', 'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio', 'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas', 'utah', 'vermont', 'virginia', 'washington', 'west virginia', 'wisconsin', 'wyoming'\n",
    "    }\n",
    "\n",
    "\n",
    "# Function to check if state exists and valid\n",
    "def format_states(state):   \n",
    "    if state.lower() in us_states:\n",
    "        return state\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe5c4fb-b0fa-450f-8623-23caf65f64a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+------------+---------------------------------------------------------------+------------+-----------------+\n|Product_ID     |Category       |Sub_Category|Product_Name                                                   |State       |Price_per_product|\n+---------------+---------------+------------+---------------------------------------------------------------+------------+-----------------+\n|FUR-CH-10002961|Furniture      |Chairs      |Leather Task Chair, Black                                      |New York    |81.882           |\n|TEC-AC-10004659|Technology     |Accessories |Imation Secure+ Hardware Encrypted USB 2.0 Flash Drive; 16GB   |Oklahoma    |72.99            |\n|OFF-BI-10002824|Office Supplies|Binders     |Recycled Easel Ring Binders                                    |Colorado    |4.25             |\n|OFF-PA-10003349|Office Supplies|Paper       |Xerox 1957                                                     |Florida     |5.184            |\n|TEC-AC-10003023|Technology     |Accessories |Logitech G105 Gaming Keyboard                                  |Ohio        |47.496           |\n|OFF-BI-10004233|Office Supplies|Binders     |\"GBC Pre-Punched Binding Paper, Plastic, White, 8-1/2\"\" x 11\"\"\"|New Jersey  |15.99            |\n|OFF-PA-10004470|Office Supplies|Paper       |\"Adams Write n' Stick Phone Message Book, 11\"\" X 5 1/4\"\"       |NULL        |NULL             |\n|FUR-FU-10001196|Furniture      |Furnishings |DAX Cubicle Frames - 8x10                                      |Indiana     |5.77             |\n|OFF-ST-10000585|Office Supplies|Storage     |Economy Rollaway Files                                         |Kentucky    |165.2            |\n|OFF-ST-10003996|Office Supplies|Storage     |Letter/Legal File Tote with Clear Snap-On Lid, Black Granite   |Washington  |16.06            |\n|OFF-FA-10002280|Office Supplies|Fasteners   |Advantus Plastic Paper Clips                                   |Pennsylvania|4.0              |\n|OFF-BI-10001989|Office Supplies|Binders     |Premium Transparent Presentation Covers by GBC                 |California  |16.784           |\n|OFF-BI-10001460|Office Supplies|Binders     |Plastic Binding Combs                                          |Arizona     |4.545            |\n|TEC-AC-10002167|Technology     |Accessories |Imation 8gb Micro Traveldrive Usb 2.0 Flash Drive              |New Jersey  |15.0             |\n|FUR-FU-10001967|Furniture      |Furnishings |Telescoping Adjustable Floor Lamp                              |New York    |19.99            |\n|FUR-FU-10003806|Furniture      |Furnishings |\"Tenex Chairmat w/ Average Lip, 45\"\" x 53\"\"\"                   |Texas       |60.4             |\n|OFF-BI-10004139|Office Supplies|Binders     |Fellowes Presentation Covers for Comb Binding Machines         |Illinois    |2.91             |\n|OFF-ST-10000991|Office Supplies|Storage     |Space Solutions HD Industrial Steel Shelving.                  |Washington  |114.97           |\n|OFF-BI-10001524|Office Supplies|Binders     |GBC Premium Transparent Covers with Diagonal Lined Pattern     |New York    |16.784           |\n|OFF-BI-10004364|Office Supplies|Binders     |Storex Dura Pro Binders                                        |Virginia    |5.94             |\n+---------------+---------------+------------+---------------------------------------------------------------+------------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Test col_name_formatter function\n",
    "def test_col_name_formatter(spark, df_product):\n",
    "    df_product = col_name_formatter(df_product)\n",
    "    expected_columns = ['Product_ID', 'Category', 'Sub_Category', 'Product_Name', 'State', 'Price_per_product']\n",
    "    assert df_product.columns == expected_columns, \"Columns are not formatted correctly\"\n",
    "    \n",
    "    return df_product\n",
    "  \n",
    "# Test casting column to DoubleType\n",
    "def test_cast_column_to_double(df_product):\n",
    "    df_product = df_product.withColumn(\"Price_per_product\", col(\"Price_per_product\").cast(DoubleType()))\n",
    "    expected_dtype = \"double\"\n",
    "    assert df_product.schema[\"Price_per_product\"].dataType.typeName() == expected_dtype, \"Price_per_product column cast to DoubleType failed\"\n",
    "    \n",
    "    return df_product\n",
    "\n",
    "# Test 1: col_name_formatter\n",
    "df_product = spark.read.option(\"delimiter\",\",\").csv(\"dbfs:/FileStore/PEI-Assessment/Product.csv\", header=True)\n",
    "df_product = test_col_name_formatter(spark, df_product)\n",
    "\n",
    "# Call UDF to format Customer_Name\n",
    "format_states_udf = udf(format_states, StringType())\n",
    "df_product = df_product.withColumn(\"State\", format_states_udf(\"State\"))\n",
    "\n",
    "# Test 2: Cast column to DoubleType\n",
    "df_product = test_cast_column_to_double(df_product)\n",
    "\n",
    "# display(df_product)\n",
    "df_product.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8674ac64-689c-4eeb-9ac9-7ce016eec321",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Question:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4619edac-823f-4c79-b72e-eab7091b1d94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_save_as_table(df_order, df_customer, df_product):    \n",
    "    # Save dataframes as Delta tables\n",
    "    df_order.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable(\"Order\")\n",
    "    df_customer.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable(\"Customer\")\n",
    "    df_product.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable(\"Product\")\n",
    "    \n",
    "    # Check if the tables are created\n",
    "    assert spark._jsparkSession.catalog().tableExists(\"Order\")\n",
    "    assert spark._jsparkSession.catalog().tableExists(\"Customer\")\n",
    "    assert spark._jsparkSession.catalog().tableExists(\"Product\")\n",
    "    \n",
    "    # Check the count of the tables\n",
    "    assert spark.sql(\"SELECT COUNT(*) FROM Order\").first()[0] == df_order.count()\n",
    "    assert spark.sql(\"SELECT COUNT(*) FROM Customer\").first()[0] == df_customer.count()\n",
    "    assert spark.sql(\"SELECT COUNT(*) FROM Product\").first()[0] == df_product.count()\n",
    "    \n",
    "    # Drop the tables\n",
    "    # spark.sql(\"DROP TABLE IF EXISTS Order\")\n",
    "    # spark.sql(\"DROP TABLE IF EXISTS Customer\")\n",
    "    # spark.sql(\"DROP TABLE IF EXISTS Product\")\n",
    "    \n",
    "    # Check if the tables are dropped\n",
    "    # assert not spark._jsparkSession.catalog().tableExists(\"Order\")\n",
    "    # assert not spark._jsparkSession.catalog().tableExists(\"Customer\")\n",
    "    # assert not spark._jsparkSession.catalog().tableExists(\"Product\")\n",
    "    \n",
    "# Run the test function\n",
    "test_save_as_table(df_order, df_customer, df_product)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "403c5965-55c1-406e-a931-3b5053c52bb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Question:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3995b866-a95d-46e3-bf51-eeb789e2709d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------------+----------+-------------------------------------------------+-----------+-------------+----------+--------------+-----------+-------+---------------+---------------+------------+-----------------------------------------------------------------------------+--------------+-----------------+\n|Customer_ID|Customer_Name|email                     |phone     |address                                          |Segment    |Country      |City      |Customer_State|Postal_Code|Region |Product_ID     |Category       |Sub_Category|Product_Name                                                                 |Product_State |Price_per_product|\n+-----------+-------------+--------------------------+----------+-------------------------------------------------+-----------+-------------+----------+--------------+-----------+-------+---------------+---------------+------------+-----------------------------------------------------------------------------+--------------+-----------------+\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |FUR-BO-10002213|Furniture      |Bookcases   |Sauder Forest Hills Library, Woodland Oak Finish                             |California    |119.833          |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |FUR-BO-10002213|Furniture      |Bookcases   |DMI Eclipse Executive Suite Bookcases                                        |New York      |400.784          |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |FUR-CH-10004086|Furniture      |Chairs      |Hon 4070 Series Pagoda Armless Upholstered Stacking Chairs                   |Arizona       |233.25           |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |OFF-PA-10003441|Office Supplies|Paper       |Xerox 226                                                                    |California    |6.48             |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |FUR-TA-10000849|Furniture      |Tables      |Bevis Rectangular Conference Tables                                          |Colorado      |72.99            |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |OFF-AR-10001940|Office Supplies|Art         |Sanford Colorific Eraseable Coloring Pencils, 12 Count                       |California    |3.28             |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |OFF-PA-10001583|Office Supplies|Paper       |\"1/4 Fold Party Design Invitations & White Envelopes, 24 8-1/2\"\" X 11\"\" Cards|NULL          |NULL             |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |TEC-PH-10003012|Technology     |Phones      |Nortel Meridian M3904 Professional Digital phone                             |Wisconsin     |153.99           |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |OFF-BI-10003527|Office Supplies|Binders     |Fellowes PB500 Electric Punch Plastic Comb Binding Machine with Manual Bind  |Maryland      |1270.99          |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |OFF-AR-10003481|Office Supplies|Art         |Newell 348                                                                   |Pennsylvania  |2.624            |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |OFF-BI-10000201|Office Supplies|Binders     |Avery Triangle Shaped Sheet Lifters, Black, 2/Pack                           |Florida       |0.738            |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |OFF-PA-10002259|Office Supplies|Paper       |\"Geographics Note Cards, Blank, White, 8 1/2\"\" x 11\"\"\"                       |Colorado      |8.75             |\n|GH-14410   |Gary Hansen  |austindyer948@gmail.com   |5424150246|00347 Murphy Unions\\nAshleyton, IA 29814         |Home Office|United States|Chicago   |Illinois      |60653      |Central|FUR-CH-10000229|Furniture      |Chairs      |Global Enterprise Series Seating High-Back Swivel/Tilt Chairs                |Texas         |189.686          |\n|GH-14410   |Gary Hansen  |austindyer948@gmail.com   |5424150246|00347 Murphy Unions\\nAshleyton, IA 29814         |Home Office|United States|Chicago   |Illinois      |60653      |Central|OFF-PA-10001289|Office Supplies|Paper       |White Computer Printout Paper by Universal                                   |Illinois      |31.008           |\n|GH-14410   |Gary Hansen  |austindyer948@gmail.com   |5424150246|00347 Murphy Unions\\nAshleyton, IA 29814         |Home Office|United States|Chicago   |Illinois      |60653      |Central|OFF-ST-10003123|Office Supplies|Storage     |Fellowes Bases and Tops For Staxonsteel/High-Stak Systems                    |California    |33.29            |\n|GH-14410   |Gary Hansen  |austindyer948@gmail.com   |5424150246|00347 Murphy Unions\\nAshleyton, IA 29814         |Home Office|United States|Chicago   |Illinois      |60653      |Central|OFF-LA-10004559|Office Supplies|Labels      |Avery 49                                                                     |North Carolina|2.304            |\n|GH-14410   |Gary Hansen  |austindyer948@gmail.com   |5424150246|00347 Murphy Unions\\nAshleyton, IA 29814         |Home Office|United States|Chicago   |Illinois      |60653      |Central|FUR-FU-10004164|Furniture      |Furnishings |Eldon 300 Class Desk Accessories, Black                                      |Washington    |4.95             |\n|GH-14410   |Gary Hansen  |austindyer948@gmail.com   |5424150246|00347 Murphy Unions\\nAshleyton, IA 29814         |Home Office|United States|Chicago   |Illinois      |60653      |Central|OFF-BI-10003291|Office Supplies|Binders     |Wilson Jones Leather-Like Binders with DublLock Round Rings                  |Virginia      |8.73             |\n|GH-14410   |Gary Hansen  |austindyer948@gmail.com   |5424150246|00347 Murphy Unions\\nAshleyton, IA 29814         |Home Office|United States|Chicago   |Illinois      |60653      |Central|OFF-ST-10001505|Office Supplies|Storage     |\"Perma STOR-ALL Hanging File Box, 13 1/8\"\"W x 12 1/4\"\"D x 10 1/2\"\"H\"         |California    |5.98             |\n|GH-14410   |Gary Hansen  |austindyer948@gmail.com   |5424150246|00347 Murphy Unions\\nAshleyton, IA 29814         |Home Office|United States|Chicago   |Illinois      |60653      |Central|FUR-FU-10003799|Furniture      |Furnishings |\"Seth Thomas 13 1/2\"\" Wall Clock\"                                            |Minnesota     |17.78            |\n+-----------+-------------+--------------------------+----------+-------------------------------------------------+-----------+-------------+----------+--------------+-----------+-------+---------------+---------------+------------+-----------------------------------------------------------------------------+--------------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_enriched_customer_products = spark.sql(\n",
    "    \"\"\"\n",
    "    Select \n",
    "    c.Customer_ID\n",
    "    ,c.Customer_Name\n",
    "    ,c.email\n",
    "    ,c.phone\n",
    "    ,c.address\n",
    "    ,c.Segment\n",
    "    ,c.Country\n",
    "    ,c.City\n",
    "    ,c.State as Customer_State\n",
    "    ,c.Postal_Code\n",
    "    ,c.Region\n",
    "    ,p.Product_ID\n",
    "    ,p.Category\n",
    "    ,p.Sub_Category\n",
    "    ,p.Product_Name\n",
    "    ,p.State as Product_State\n",
    "    ,p.Price_per_product\n",
    "    -- ,o.Order_ID\n",
    "    from customer as c\n",
    "    join `order` as o\n",
    "    join product as p\n",
    "    on c.Customer_ID = o.Customer_ID\n",
    "    and p.Product_ID = o.Product_ID\n",
    "    \"\"\")\n",
    "\n",
    "# display(df_enriched_customer_products)\n",
    "df_enriched_customer_products.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c58685f4-0a6b-472c-af19-69c422ff4120",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_enriched_customer_products_table():\n",
    "    # Save dataframe as Delta table\n",
    "    df_enriched_customer_products.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable(\"enriched_customer_products\")\n",
    "\n",
    "    # Test case: check if the table exists\n",
    "    table_exists = spark._jsparkSession.catalog().tableExists(\"enriched_customer_products\")\n",
    "    assert table_exists\n",
    "\n",
    "    # Test case: check if the table has correct schema\n",
    "    expected_schema = [\n",
    "        'Customer_ID', 'Customer_Name', 'email', 'phone', 'address', 'Segment', 'Country', 'City', 'Customer_State',\n",
    "        'Postal_Code', 'Region', 'Product_ID', 'Category', 'Sub_Category', 'Product_Name', 'Product_State',\n",
    "        'Price_per_product'\n",
    "    ]\n",
    "    actual_schema = [field.name for field in spark.catalog.listColumns(\"enriched_customer_products\")]\n",
    "    assert actual_schema == expected_schema\n",
    "\n",
    "    # Test case: check if the table has data\n",
    "    row_count = spark.sql(\"SELECT COUNT(*) FROM enriched_customer_products\").collect()[0][0]\n",
    "    assert row_count > 0\n",
    "\n",
    "test_enriched_customer_products_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4298050-318a-4dcc-a06a-23a54ad03c5a",
     "showTitle": true,
     "title": "Duplicates on Product ID was observed"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Product_ID</th><th>Category</th><th>Sub_Category</th><th>Product_Name</th><th>State</th><th>Price_per_product</th></tr></thead><tbody><tr><td>FUR-CH-10002961</td><td>Furniture</td><td>Chairs</td><td>Leather Task Chair, Black</td><td>New York</td><td>81.882</td></tr><tr><td>FUR-CH-10002961</td><td>Furniture</td><td>Chairs</td><td>Leather Task Chair, Black</td><td>Pennsylvania</td><td>63.686</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "FUR-CH-10002961",
         "Furniture",
         "Chairs",
         "Leather Task Chair, Black",
         "New York",
         81.882
        ],
        [
         "FUR-CH-10002961",
         "Furniture",
         "Chairs",
         "Leather Task Chair, Black",
         "Pennsylvania",
         63.686
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Product_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sub_Category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Product_Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "State",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Price_per_product",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Duplicates on Product ID was observed\n",
    "display(spark.sql(\"\"\"\n",
    "          select * from product\n",
    "          where Product_ID='FUR-CH-10002961'\n",
    "          \"\"\"))\n",
    "# duplicates in product id observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be760dbc-8bba-45d1-8e92-6368b4439e72",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Question:3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d4ee097-7bef-45f3-bc99-3ea21fc124ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+------+---------------+------------+\n|Customer_Name     |Country      |Profit|Category       |Sub_Category|\n+------------------+-------------+------+---------------+------------+\n|Jay Kimmel        |United States|63.69 |Furniture      |Chairs      |\n|Jay Kimmel        |United States|63.69 |Furniture      |Chairs      |\n|Bil Donatelli     |United States|102.19|Technology     |Accessories |\n|Laurel Beltran    |United States|-14.92|Office Supplies|Binders     |\n|Karl Braun        |United States|5.64  |Office Supplies|Paper       |\n|Denny Ordway      |United States|-3.0  |Technology     |Accessories |\n|Cassandra Brandow |United States|38.38 |Office Supplies|Binders     |\n|Sally Matthias    |United States|5.23  |Office Supplies|Paper       |\n|Rick Duston       |United States|5.19  |Furniture      |Furnishings |\n|Justin MacKendrick|United States|214.0 |Office Supplies|Storage     |\n|Scot Coram        |United States|4.18  |Office Supplies|Storage     |\n|Bil Overfelt      |United States|2.8   |Office Supplies|Fasteners   |\n|Bil Donatelli     |United States|41.12 |Office Supplies|Binders     |\n|Adrian Barton     |United States|-13.94|Office Supplies|Binders     |\n|Nra Paige         |United States|4.0   |Technology     |Accessories |\n|Karen Danels      |United States|10.0  |Furniture      |Furnishings |\n|Barry Franz       |United States|-378.4|Furniture      |Furnishings |\n|Laurel Elliston   |United States|-30.56|Office Supplies|Binders     |\n|Ted Trevino       |United States|10.35 |Office Supplies|Storage     |\n|Cthy Armstrong    |United States|47.0  |Office Supplies|Binders     |\n+------------------+-------------+------+---------------+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_enriched_order_customer_product = spark.sql(\n",
    "    \"\"\"\n",
    "    Select \n",
    "    c.Customer_Name\n",
    "    ,c.Country\n",
    "    ,Round(o.Profit, 2) as Profit\n",
    "    ,p.Category\n",
    "    ,p.Sub_Category\n",
    "    from `order` as o\n",
    "    join customer as c\n",
    "    join product as p\n",
    "    on c.Customer_ID = o.Customer_ID\n",
    "    and p.Product_ID = o.Product_ID\n",
    "    \"\"\")\n",
    "    \n",
    "# display(df_enriched_order_customer_product)\n",
    "df_enriched_order_customer_product.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "328c2585-8c57-4755-b150-84179b6d5cb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_enriched_order_customer_product_table():\n",
    "    # Save dataframe as Delta table\n",
    "    df_enriched_order_customer_product.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable(\"enriched_order_customer_product\")\n",
    "\n",
    "    # Test case: check if the table exists\n",
    "    table_exists = spark._jsparkSession.catalog().tableExists(\"enriched_order_customer_product\")\n",
    "    assert table_exists\n",
    "\n",
    "    # Test case: check if the table has correct schema\n",
    "    expected_schema = [\"Customer_Name\", \"Country\", \"Profit\", \"Category\", \"Sub_Category\"]\n",
    "    actual_schema = [field.name for field in spark.catalog.listColumns(\"enriched_order_customer_product\")]\n",
    "    assert actual_schema == expected_schema\n",
    "\n",
    "    # Test case: check if the table has data\n",
    "    row_count = spark.sql(\"SELECT COUNT(*) FROM enriched_order_customer_product\").collect()[0][0]\n",
    "    assert row_count > 0\n",
    "\n",
    "test_enriched_order_customer_product_table()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2680cefe-bddc-4192-99f2-5503aacb71c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Question:4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fd4293c-0e10-491d-a417-81321f092525",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+---------------+------------+-----------+--------------------+\n|Aggregate_Profit |Year|Category       |Sub_Category|Customer_ID|Customer_Name       |\n+-----------------+----+---------------+------------+-----------+--------------------+\n|133.0            |2017|Furniture      |Furnishings |MH-17290   |Marc Harrigan       |\n|99.9012          |2017|Office Supplies|Appliances  |BM-11140   |Becky Martin        |\n|5.1042           |2017|Office Supplies|Envelopes   |CC-12220   |Chris Cortes        |\n|2.0416           |2017|Office Supplies|Art         |TA-21385   |Tom Ashbrook        |\n|11.998           |2017|Technology     |Accessories |HP-14815   |Harold Pawlan       |\n|-32.2192         |2017|Furniture      |Bookcases   |DW-13585   |Dorothy Wardle      |\n|107.9892         |2017|Technology     |Accessories |CS-12355   |Christine Sundaresam|\n|268.347          |2017|Office Supplies|Paper       |TH-21550   |Tracy Hopkins       |\n|170.2833         |2017|Office Supplies|Storage     |MK-18160   |Mike Kennedy        |\n|-66.062          |2017|Furniture      |Furnishings |RR-19315   |Ralph Ritter        |\n|2.891            |2017|Office Supplies|Art         |JL-15130   |Jack Lebron         |\n|87.7578          |2017|Office Supplies|Storage     |BD-11725   |Bruce Degenhardt    |\n|4.1391           |2017|Technology     |Phones      |KW-16435   |Katrina Willman     |\n|8.0968           |2017|Furniture      |Furnishings |JO-15145   |Jack O'Briant       |\n|18.6006          |2017|Office Supplies|Binders     |JE-15475   |Jeremy Ellison      |\n|115.1856         |2017|Technology     |Accessories |CR-12820   |Cyra Reiten         |\n|9.516399999999999|2017|Office Supplies|Paper       |HZ-14950   |Henia Zydlo         |\n|361.2994         |2017|Office Supplies|Appliances  |NH-18610   |Nicole Hansen       |\n|-50.396          |2017|Technology     |Phones      |ES-14080   |Erin Smith          |\n|5.8065           |2017|Technology     |Accessories |FW-14395   |Fred Waerman        |\n+-----------------+----+---------------+------------+-----------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_profit_by_year_pc_psc_customer = spark.sql(\"\"\"\n",
    "                  Select \n",
    "                  sum(Profit) as Aggregate_Profit\n",
    "                  ,year(o.Order_Date) as Year\n",
    "                  ,p.Category\n",
    "                  ,p.Sub_Category\n",
    "                  ,c.Customer_ID\n",
    "                  ,c.Customer_Name\n",
    "                  from `order` as o\n",
    "                  join product as p\n",
    "                  join customer as c\n",
    "                  on c.Customer_ID = o.Customer_ID\n",
    "                  and p.Product_ID = o.Product_ID \n",
    "                  group by\n",
    "                  year(o.Order_Date)\n",
    "                  ,p.Category\n",
    "                  ,p.Sub_Category\n",
    "                  ,c.Customer_ID\n",
    "                  ,c.Customer_Name\n",
    "                  order by Year desc\n",
    "                  \"\"\")\n",
    "\n",
    "# display(df_profit_by_year_pc_psc_customer)\n",
    "df_profit_by_year_pc_psc_customer.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0722d2e0-5869-40f4-a20b-c74a2032e1c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_profit_by_year_pc_psc_customer_table():\n",
    "    # Save dataframe as Delta table\n",
    "    df_profit_by_year_pc_psc_customer.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable(\"profit_by_year_pc_psc_customer\")\n",
    "\n",
    "    # Test case: check if the table exists\n",
    "    table_exists = spark._jsparkSession.catalog().tableExists(\"profit_by_year_pc_psc_customer\")\n",
    "    assert table_exists\n",
    "\n",
    "    # Test case: check if the table has correct schema\n",
    "    expected_schema = [\"Aggregate_Profit\", \"Year\", \"Category\", \"Sub_Category\", \"Customer_ID\", \"Customer_Name\"]\n",
    "    actual_schema = [field.name for field in spark.catalog.listColumns(\"profit_by_year_pc_psc_customer\")]\n",
    "    assert actual_schema == expected_schema\n",
    "\n",
    "    # Test case: check if the table has data\n",
    "    row_count = spark.sql(\"SELECT COUNT(*) FROM profit_by_year_pc_psc_customer\").collect()[0][0]\n",
    "    assert row_count > 0\n",
    "\n",
    "test_profit_by_year_pc_psc_customer_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b07c6e38-20f6-4575-8f20-6c2f5fdec5e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Question:5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2075b4d9-8de9-431b-aff6-f74681cf50e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+\n|Profit_By_Year    |Year|\n+------------------+----+\n|127175.11320000011|2017|\n|68161.4049000001  |2016|\n|65706.34270000012 |2015|\n|40975.45719999994 |2014|\n+------------------+----+\n\n"
     ]
    }
   ],
   "source": [
    "def test_profit_by_year():\n",
    "    # Run SQL Query\n",
    "    df_profit_by_year = spark.sql(\n",
    "      \"\"\"\n",
    "      Select \n",
    "      sum(Aggregate_Profit) as Profit_By_Year\n",
    "      ,Year\n",
    "      from profit_by_year_pc_psc_customer\n",
    "      group by\n",
    "      Year\n",
    "      order by Year desc\n",
    "      \"\"\")\n",
    "    # display(df_profit_by_year)\n",
    "    df_profit_by_year.show(truncate=False)\n",
    "\n",
    "    # Test case: check if the dataframe has correct schema\n",
    "    expected_schema = [\"Profit_By_Year\", \"Year\"]\n",
    "    actual_schema = [field for field in df_profit_by_year.columns]\n",
    "    assert actual_schema == expected_schema\n",
    "\n",
    "    # Test case: check if the dataframe has data\n",
    "    assert df_profit_by_year.select('Year').distinct().count() == spark.sql(\"Select count(distinct year(Order_Date)) from order\").collect()[0][0]\n",
    "\n",
    "test_profit_by_year()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1762d683-3c9f-49bd-bc26-ceed952ce6b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Question:5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "142e0bb6-4fa0-45cb-9262-93aaf0a0894b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----+----------------+\n|Profit_By_Year_And_Prd_Cat|Year|Product_Category|\n+--------------------------+----+----------------+\n|45330.59050000009         |2017|Office Supplies |\n|78482.83109999997         |2017|Technology      |\n|3361.6915999999983        |2017|Furniture       |\n|24437.399599999986        |2016|Technology      |\n|7750.212200000004         |2016|Furniture       |\n|35973.79310000002         |2016|Office Supplies |\n|25490.43370000002         |2015|Office Supplies |\n|3392.1574                 |2015|Furniture       |\n|36823.75160000004         |2015|Technology      |\n|23486.18540000001         |2014|Technology      |\n|-5174.653799999998        |2014|Furniture       |\n|22663.92559999998         |2014|Office Supplies |\n+--------------------------+----+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def test_profit_by_year_cat():\n",
    "    # Run SQL Query\n",
    "    df_profit_by_year_cat = spark.sql(\n",
    "      \"\"\"\n",
    "      Select \n",
    "      sum(Aggregate_Profit) as Profit_By_Year_And_Prd_Cat\n",
    "      ,Year\n",
    "      ,Category as Product_Category\n",
    "      from profit_by_year_pc_psc_customer\n",
    "      group by\n",
    "      Year\n",
    "      ,Product_Category\n",
    "      order by Year desc\n",
    "      \"\"\")\n",
    "    # display(df_profit_by_year_cat)\n",
    "    df_profit_by_year_cat.show(truncate=False)\n",
    "\n",
    "    # Test case: check if the dataframe has correct schema\n",
    "    expected_schema = [\"Profit_By_Year_And_Prd_Cat\", \"Year\", \"Product_Category\"]\n",
    "    actual_schema = [field for field in df_profit_by_year_cat.columns]\n",
    "    assert actual_schema == expected_schema\n",
    "\n",
    "    # Test case: check if the dataframe has data\n",
    "    assert df_profit_by_year_cat.select('Year','Product_Category').distinct().count() == spark.sql(\"Select count(distinct Year, Category) from profit_by_year_pc_psc_customer\").collect()[0][0]\n",
    "\n",
    "test_profit_by_year_cat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f6883e6-68ca-4329-a7f8-b189cae44b48",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Question:5c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d51422bd-0ca3-4b84-9288-e6b1fa1920fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+--------------------+\n|Profit_By_Customer |Customer_ID|Customer_Name       |\n+-------------------+-----------+--------------------+\n|-273.40890000000013|AA-10315   |Alex Avila          |\n|277.3824           |AA-10375   |Allen Armold        |\n|445.96940000000006 |AA-10480   |Andrew Allen        |\n|807.8329           |AA-10645   |Anna Andreadi       |\n|129.6821           |AB-10015   |Aaron Bergman       |\n|2047.8491000000001 |AB-10060   |Adam Bellavance     |\n|5483.749           |AB-10105   |Adrian Barton       |\n|320.6815           |AB-10150   |Aimee Bixby         |\n|215.36410000000004 |AB-10165   |Alan Barnes         |\n|264.56749999999994 |AB-10255   |Alejandro Ballentine|\n|-275.286           |AB-10600   |Ann Blume           |\n|-62.1342           |AC-10420   |Alyssa Crouse       |\n|1730.0927000000001 |AC-10450   |Amy Cox             |\n|298.61330000000004 |AC-10615   |Ann Chong           |\n|-28.700399999999995|AC-10660   |Anna Chung          |\n|1869.9293999999998 |AD-10180   |Alan Dominguez      |\n|317.63360000000006 |AF-10870   |Art Ferguson        |\n|-163.1175          |AF-10885   |Art Foster          |\n|732.7399000000001  |AG-10270   |Alejandro Grove     |\n|59.2884            |AG-10300   |Aleksandra Gannaway |\n+-------------------+-----------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "def test_profit_by_customer():\n",
    "    # Run SQL Query\n",
    "    df_profit_by_customer = spark.sql(\n",
    "      \"\"\"\n",
    "      Select \n",
    "      sum(Aggregate_Profit) as Profit_By_Customer\n",
    "      ,Customer_ID\n",
    "      ,Customer_Name\n",
    "      from profit_by_year_pc_psc_customer\n",
    "      group by\n",
    "      Customer_ID\n",
    "      ,Customer_Name\n",
    "      order by Customer_ID\n",
    "      \"\"\")\n",
    "    # display(df_profit_by_customer)\n",
    "    df_profit_by_customer.show(truncate=False)\n",
    "\n",
    "    # Test case: check if the dataframe has correct schema\n",
    "    expected_schema = [\"Profit_By_Customer\", \"Customer_ID\", \"Customer_Name\"]\n",
    "    actual_schema = [field for field in df_profit_by_customer.columns]\n",
    "    assert actual_schema == expected_schema\n",
    "\n",
    "    # Test case: check if the dataframe has data\n",
    "    assert df_profit_by_customer.select('Customer_ID').distinct().count() == spark.sql(\"Select count(distinct Customer_ID) from profit_by_year_pc_psc_customer\").collect()[0][0]\n",
    "\n",
    "test_profit_by_customer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3b2699f-442e-409a-9d57-5b0ec35e8bc2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Question:5d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcbf94c4-c99c-475a-a862-cc7950ae8f84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+----+-----------+------------------+\n|Profit_By_Year_And_Customer|Year|Customer_ID|Customer_Name     |\n+---------------------------+----+-----------+------------------+\n|10.4754                    |2017|EM-13810   |NULL              |\n|33.7788                    |2017|CC-12610   |Corey Catlett     |\n|221.47639999999998         |2017|MK-18160   |Mike Kennedy      |\n|74.228                     |2017|CB-12415   |Christy Brittain  |\n|108.05359999999999         |2017|MH-17620   |Matt Hagelstein   |\n|397.7298                   |2017|MG-18145   |Mike Gockenbach   |\n|1210.7213                  |2017|KF-16285   |Karen Ferguson    |\n|36.9728                    |2017|JF-15490   |Jeremy Farry      |\n|423.7296                   |2017|EJ-13720   |Ed Jacobs         |\n|56.8938                    |2017|EP-13915   |Emily Phan        |\n|196.45880000000002         |2017|VP-21730   |Victor Preis      |\n|-83.4279                   |2017|SC-20770   |Stewart Carmichael|\n|164.91330000000002         |2017|CL-12700   |Craig Leslie      |\n|871.8167                   |2017|ES-14020   |Erica Smith       |\n|185.9474                   |2017|CM-11815   |Candace McMahon   |\n|6780.896300000001          |2017|RB-19360   |Raymond Buch      |\n|110.52130000000001         |2017|DM-13015   |Darrin Martin     |\n|-2330.2698                 |2017|NC-18415   |Nathan Cano       |\n|116.4476                   |2017|MP-17965   |Michael Paige     |\n|11.0594                    |2017|DC-13285   |Debra Catini      |\n+---------------------------+----+-----------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "def test_profit_by_year_customer():\n",
    "    # Run SQL Query\n",
    "    df_profit_by_year_customer = spark.sql(\n",
    "      \"\"\"\n",
    "      Select \n",
    "      sum(Aggregate_Profit) as Profit_By_Year_And_Customer\n",
    "      ,Year\n",
    "      ,Customer_ID\n",
    "      ,Customer_Name\n",
    "      from profit_by_year_pc_psc_customer\n",
    "      group by\n",
    "      Year\n",
    "      ,Customer_ID\n",
    "      ,Customer_Name\n",
    "      order by Year desc\n",
    "      \"\"\")\n",
    "    # display(df_profit_by_year_customer)\n",
    "    df_profit_by_year_customer.show(truncate=False)\n",
    "\n",
    "    # Test case: check if the dataframe has correct schema\n",
    "    expected_schema = [\"Profit_By_Year_And_Customer\", \"Year\", \"Customer_ID\", \"Customer_Name\"]\n",
    "    actual_schema = [field for field in df_profit_by_year_customer.columns]\n",
    "    assert actual_schema == expected_schema\n",
    "\n",
    "    # Test case: check if the dataframe has data\n",
    "    assert df_profit_by_year_customer.select('Year', 'Customer_ID').distinct().count() == spark.sql(\"Select count(distinct Year, Customer_ID) from profit_by_year_pc_psc_customer\").collect()[0][0]\n",
    "\n",
    "test_profit_by_year_customer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81be34f3-c64c-484d-81ae-cfb732ec0841",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PEI-Assessment_Lite",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
